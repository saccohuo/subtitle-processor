from flask import Flask, request, jsonify
import os
import logging
import sys
import time
import soundfile as sf
import numpy as np
from funasr import AutoModel
import torch
import json
from pydub import AudioSegment
import tempfile
from modelscope import snapshot_download
from pathlib import Path
import shutil
from datetime import datetime
import errno

# FunASR æ¨¡å‹åˆ—è¡¨
FUNASR_MODELS = [
    "SenseVoiceSmall", "paraformer-zh", "paraformer-zh-streaming", "paraformer-en",
    "conformer-en", "ct-punc", "fsmn-vad", "fsmn-kws", "fa-zh", "cam++",
    "Qwen-Audio", "Qwen-Audio-Chat", "emotion2vec+large"
]

MODEL_MAPPINGS = {
    "main": {
        "paraformer-zh": "damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
        "paraformer-zh-streaming": "damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
        "paraformer-zh-vad-punc": "iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
        "paraformer-en": "damo/speech_paraformer-large_asr_nat-en-16k-common-vocab10020",
        "conformer-en": "damo/speech_conformer_asr_nat-en-16k-common-vocab10020",
        "SenseVoiceSmall": "damo/speech_SenseVoiceSmall_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
        "fa-zh": "damo/speech_FastConformer_asr_nat-zh-cn-16k-common-vocab8404",
        "Qwen-Audio": "damo/speech_qwen_audio_asr_nat-zh-cn-16k-common-vocab8404",
        "Qwen-Audio-Chat": "damo/speech_qwen_audio_chat_asr_nat-zh-cn-16k-common-vocab8404",
        "emotion2vec+large": "damo/speech_emotion2vec_large_sv_zh-cn_16k-common",
    },
    "vad": {
        "fsmn-vad": "damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
        "fsmn-kws": "damo/speech_fsmn_kws_zh-cn-16k-common-pytorch",
    },
    "punc": {
        "ct-punc": "damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
    },
    "spk": {
        "cam++": "damo/speech_campplus_sv_zh-cn_16k-common",
    },
}

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

# åˆ›å»ºlogger
logger = logging.getLogger("transcribe-audio")
logger.setLevel(logging.DEBUG)

# æ—¥å¿—æ–‡ä»¶è¾“å‡º
log_dir = Path(os.getenv("LOG_DIR", "/app/logs"))
log_dir.mkdir(parents=True, exist_ok=True)
log_file = log_dir / os.getenv("LOG_FILE", "transcribe-audio.log")
file_handler = logging.FileHandler(log_file, encoding="utf-8")
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(
    logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
)
logger.addHandler(file_handler)
logging.getLogger().addHandler(file_handler)
logger.info("æ—¥å¿—æ–‡ä»¶è¾“å‡ºåˆ° %s", log_file)

# ç¡®ä¿å…¶ä»–åº“çš„æ—¥å¿—çº§åˆ«ä¸ä¼šå¤ªè¯¦ç»†
logging.getLogger("modelscope").setLevel(logging.ERROR)
logging.getLogger("funasr").setLevel(logging.ERROR)
logging.getLogger("jieba").setLevel(logging.ERROR)
logging.getLogger("werkzeug").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("filelock").setLevel(logging.WARNING)

# å¤„ç† macOS ä¸Šå¶å‘çš„ EDEADLK æ­»é”ï¼Œé‡è¯•åŠ è½½ FunASR åˆ†è¯å­—å…¸
try:
    from funasr.tokenizer import char_tokenizer as _funasr_char_tokenizer
except ImportError:
    _funasr_char_tokenizer = None
else:
    if not getattr(_funasr_char_tokenizer.load_seg_dict, "__name__", "").startswith("_load_seg_dict_with_retry"):
        _original_load_seg_dict = _funasr_char_tokenizer.load_seg_dict

        def _load_seg_dict_with_retry(seg_dict, max_retries=5, base_delay=0.5):
            """Wrap funasr load_seg_dict to mitigate occasional deadlocks on shared volumes."""
            last_err = None
            for attempt in range(1, max_retries + 1):
                try:
                    return _original_load_seg_dict(seg_dict)
                except OSError as err:
                    last_err = err
                    if err.errno != errno.EDEADLK or attempt == max_retries:
                        raise
                    wait_time = base_delay * attempt
                    logger.warning(
                        "åŠ è½½åˆ†è¯å­—å…¸æ—¶é‡åˆ°ç³»ç»Ÿ EDEADLK æ­»é”ï¼Œæ­£åœ¨é‡è¯• (%s/%s)ï¼Œ%.1f ç§’åç»§ç»­: %s",
                        attempt,
                        max_retries,
                        wait_time,
                        err,
                    )
                    time.sleep(wait_time)
            raise last_err

        _load_seg_dict_with_retry.__wrapped__ = _original_load_seg_dict
        _funasr_char_tokenizer.load_seg_dict = _load_seg_dict_with_retry

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024  # 500MB max-limit
app.config['UPLOAD_FOLDER'] = '/app/uploads'


# å…¨å±€æ¨¡å‹å˜é‡
model = None

# å…¨å±€è¿›åº¦è·Ÿè¸ª
current_progress = {
    "status": "idle",
    "progress": 0,
    "total_chunks": 0,
    "current_chunk": 0,
    "message": "ç­‰å¾…å¤„ç†...",
    "start_time": None,
    "estimated_time": None
}

# è®¾ç½®è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆ5åˆ†é’Ÿï¼‰
app.config['TIMEOUT'] = 300

def ensure_dir(dir_path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    Path(dir_path).mkdir(parents=True, exist_ok=True)

def cleanup_model_locks(cache_dir):
    """ç§»é™¤é—ç•™çš„æ¨¡å‹é”æ–‡ä»¶ï¼Œé˜²æ­¢æ¨¡å‹ä¸‹è½½å› æ–‡ä»¶é”å¡æ­»"""
    cache_path = Path(cache_dir)
    if not cache_path.exists():
        return
    for pattern in ("**/.mdl", "**/*.mdl.lock"):
        for lock_file in cache_path.glob(pattern):
            if lock_file.is_file():
                try:
                    lock_file.unlink()
                    logger.warning(f"ç§»é™¤äº†é—ç•™çš„æ¨¡å‹é”æ–‡ä»¶: {lock_file}")
                except OSError as err:
                    logger.warning(f"æ— æ³•ç§»é™¤æ¨¡å‹é”æ–‡ä»¶ {lock_file}: {err}")
    for lock_dir in cache_path.glob("**/.lock"):
        if lock_dir.is_dir():
            try:
                shutil.rmtree(lock_dir)
                logger.warning(f"ç§»é™¤äº†é—ç•™çš„æ¨¡å‹é”ç›®å½•: {lock_dir}")
            except OSError as err:
                logger.warning(f"æ— æ³•ç§»é™¤æ¨¡å‹é”ç›®å½• {lock_dir}: {err}")
    for temp_dir in cache_path.glob("**/._____temp"):
        if temp_dir.is_dir():
            try:
                shutil.rmtree(temp_dir)
                logger.warning(f"ç§»é™¤äº†é—ç•™çš„ä¸´æ—¶ç›®å½•: {temp_dir}")
            except OSError as err:
                logger.warning(f"æ— æ³•ç§»é™¤ä¸´æ—¶ç›®å½• {temp_dir}: {err}")

def download_model(model_id, revision, cache_dir):
    """ä¸‹è½½æŒ‡å®šçš„æ¨¡å‹"""
    try:
        logger.info(f"å¼€å§‹ä¸‹è½½æ¨¡å‹ {model_id} åˆ° {cache_dir}")
        cleanup_model_locks(cache_dir)
        specific_lock_files = [
            Path(cache_dir) / model_id / ".mdl",
            Path(cache_dir) / "hub" / model_id / ".mdl",
        ]
        for lock_path in specific_lock_files:
            if lock_path.exists():
                try:
                    lock_path.unlink()
                    logger.warning(f"ä¸‹è½½å‰ç§»é™¤é”æ–‡ä»¶: {lock_path}")
                except OSError as err:
                    logger.warning(f"ç§»é™¤é”æ–‡ä»¶ {lock_path} å¤±è´¥: {err}")
        # ç§»é™¤æœªå®Œæ•´ä¸‹è½½çš„ä¸´æ—¶ç›®å½•
        for stale_path in [
            Path(cache_dir) / "._____temp",
            Path(cache_dir) / "hub" / "._____temp",
            Path(cache_dir) / model_id / "._____temp",
            Path(cache_dir) / "hub" / model_id / "._____temp",
        ]:
            if stale_path.exists():
                try:
                    shutil.rmtree(stale_path)
                    logger.warning(f"ç§»é™¤äº†é—ç•™çš„æ¨¡å‹ä¸´æ—¶ç›®å½•: {stale_path}")
                except OSError as err:
                    logger.warning(f"æ— æ³•ç§»é™¤æ¨¡å‹ä¸´æ—¶ç›®å½• {stale_path}: {err}")
        
        # ä¸‹è½½æ¨¡å‹
        model_dir = snapshot_download(
            model_id=model_id,
            revision=revision,
            cache_dir=cache_dir
        )
        logger.info(f"æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_dir}")
        
        # ç¡®ä¿ç›®å½•æœ‰å†™æƒé™
        for root, dirs, files in os.walk(cache_dir):
            for d in dirs:
                os.chmod(os.path.join(root, d), 0o755)
            for f in files:
                os.chmod(os.path.join(root, f), 0o644)
        
        return model_dir
    except Exception as e:
        logger.error(f"ä¸‹è½½æ¨¡å‹ {model_id} æ—¶å‡ºé”™: {str(e)}")
        raise

def get_model_id(model_type, model_name):
    """è·å–å®Œæ•´çš„æ¨¡å‹ID"""
    model_mappings = MODEL_MAPPINGS

    # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æ¨¡å‹IDï¼ˆåŒ…å«ä»“åº“å‰ç¼€ï¼‰
    if "/" in model_name:
        return model_name
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æ˜ å°„è¡¨ä¸­
    try:
        return model_mappings[model_type][model_name]
    except KeyError:
        # å¦‚æœæ‰¾ä¸åˆ°æ˜ å°„ï¼Œå°è¯•æ„å»ºæ ‡å‡†æ ¼å¼çš„æ¨¡å‹ID
        if model_type == "main":
            # ä¸»æ¨¡å‹IDæ ¼å¼ï¼šdamo/speech_[model_name]_asr_nat-[lang]-16k-common-[vocab]
            if "zh" in model_name.lower():
                return f"damo/speech_{model_name}_asr_nat-zh-cn-16k-common-vocab8404-pytorch"
            elif "en" in model_name.lower():
                return f"damo/speech_{model_name}_asr_nat-en-16k-common-vocab10020"
            else:
                return f"damo/speech_{model_name}_asr_nat-zh-cn-16k-common-vocab8404-pytorch"
        elif model_type == "vad":
            # VADæ¨¡å‹IDæ ¼å¼ï¼šdamo/speech_[model_name]_zh-cn-16k-common-pytorch
            return f"damo/speech_{model_name}_zh-cn-16k-common-pytorch"
        elif model_type == "punc":
            # æ ‡ç‚¹æ¨¡å‹IDæ ¼å¼ï¼šdamo/punc_[model_name]_zh-cn-common-vocab272727-pytorch
            return f"damo/punc_{model_name}_zh-cn-common-vocab272727-pytorch"
        elif model_type == "spk":
            # è¯´è¯äººåˆ†ç¦»æ¨¡å‹IDæ ¼å¼ï¼šdamo/speech_[model_name]_sv_zh-cn_16k-common
            return f"damo/speech_{model_name}_sv_zh-cn_16k-common"
        else:
            # å¦‚æœæ— æ³•ç¡®å®šæ ¼å¼ï¼Œç›´æ¥æ·»åŠ damo/å‰ç¼€
            logger.warning(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}ï¼Œå°†ç›´æ¥æ·»åŠ damo/å‰ç¼€")
            return f"damo/{model_name}"


def resolve_model_config(model_type: str, model_name: str):
    """æ ‡å‡†åŒ–æ¨¡å‹é…ç½®ï¼Œè¿”å›è¿è¡Œæ—¶æ‰€éœ€åç§°ä¸ID."""
    normalized = (model_name or "").strip()
    mapping = MODEL_MAPPINGS.get(model_type, {})
    requires_trust_remote_code = False

    if "/" in normalized:
        alias = next((alias for alias, full_id in mapping.items() if full_id == normalized), None)
        if alias:
            runtime_name = alias
        else:
            runtime_name = normalized
            requires_trust_remote_code = True
    else:
        runtime_name = normalized

    model_id = get_model_id(model_type, normalized)

    return {
        "name": normalized or runtime_name,
        "id": model_id,
        "runtime": runtime_name,
        "requires_trust_remote_code": requires_trust_remote_code,
    }

def ensure_models():
    """ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸‹è½½"""
    # è¿è¡Œæ—¶ç›®å½•ä¸å…±äº«ç›®å½•è§£è€¦ï¼Œé¿å…å…±äº«å·ä¸Šçš„æ–‡ä»¶é”é—®é¢˜
    runtime_dir = Path(os.getenv("MODEL_DIR", "/app/runtime-models"))
    ensure_dir(runtime_dir)
    model_dir = str(runtime_dir)

    cleanup_model_locks(model_dir)

    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['MODELSCOPE_CACHE'] = model_dir
    os.environ['HF_HOME'] = model_dir
    os.environ['TORCH_HOME'] = model_dir

    # è·å–æ‰€æœ‰æ¨¡å‹åç§° - ä¼˜å…ˆä½¿ç”¨æ”¯æŒç¬¬ä¸‰ä»£çƒ­è¯çš„æ¨¡å‹
    model_name = os.getenv("FUNASR_MODEL", "paraformer-zh").strip()
    vad_model = os.getenv("FUNASR_VAD_MODEL", "fsmn-vad").strip()
    punc_model = os.getenv("FUNASR_PUNC_MODEL", "ct-punc").strip()
    spk_model = os.getenv("FUNASR_SPK_MODEL", "").strip()
    
    # è·å–å®Œæ•´çš„æ¨¡å‹ID
    model_configs = {
        "main": resolve_model_config("main", model_name) if model_name else None,
        "vad": resolve_model_config("vad", vad_model) if vad_model else None,
        "punc": resolve_model_config("punc", punc_model) if punc_model else None,
        "spk": resolve_model_config("spk", spk_model) if spk_model else None,
    }
    
    logger.info("æ£€æŸ¥æ¨¡å‹é…ç½®ï¼š")
    for model_type, config in model_configs.items():
        if not config:
            logger.info(f"{model_type}æ¨¡å‹: å·²ç¦ç”¨")
            continue
        logger.info(f"{model_type}æ¨¡å‹: {config['runtime']} (ID: {config['id']})")
    
    # æ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    def _model_exists(model_id: str) -> bool:
        candidates = [
            Path(model_dir) / "hub" / model_id,
            Path(model_dir) / model_id,
            Path(model_dir) / "models" / model_id,
        ]
        return any(path.exists() for path in candidates)
    
    model_paths = {
        model_type: [
            str(path) for path in [
                Path(model_dir) / "hub" / config["id"],
                Path(model_dir) / config["id"],
                Path(model_dir) / "models" / config["id"],
            ]
        ]
        for model_type, config in model_configs.items() if config
    }

    logger.info("æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼š")
    for model_type, paths in model_paths.items():
        logger.info(f"{model_type}æ¨¡å‹å€™é€‰è·¯å¾„: {paths}")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½æ¨¡å‹
    missing_models = [
        (model_type, config)
        for model_type, config in model_configs.items()
        if config and not _model_exists(config["id"])
    ]
    if missing_models:
        missing_desc = [f"{model_type}:{config['id']}" for model_type, config in missing_models]
        logger.info("ç¼ºå¤±æ¨¡å‹åˆ—è¡¨: %s", missing_desc)
        logger.info("éƒ¨åˆ†æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
        ensure_dir(model_dir)

        # ä¸‹è½½æ‰€æœ‰ç¼ºå¤±çš„æ¨¡å‹
        success = True
        for model_type, config in missing_models:
            try:
                downloaded_path = download_model(
                    model_id=config["id"],
                    revision=None,  # ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
                    cache_dir=model_dir
                )
                logger.info(f"æ¨¡å‹ {config['id']} ä¸‹è½½æˆåŠŸ: {downloaded_path}")
            except Exception as e:
                logger.error(f"ä¸‹è½½æ¨¡å‹ {config['id']} å¤±è´¥: {str(e)}")
                success = False
                continue

        if not success:
            logger.error("éƒ¨åˆ†æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•")
            sys.exit(1)
        else:
            logger.info(f"æ‰€æœ‰æ¨¡å‹ä¸‹è½½æˆåŠŸï¼Œæ¨¡å‹ç›®å½•: {model_dir}")
    else:
        logger.info("æ‰€æœ‰æ¨¡å‹æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½")
    
    timestamp_capable = {
        "iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
        "iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
    }
    supports_timestamp = model_configs["main"]["id"] in timestamp_capable

    if not supports_timestamp:
        logger.warning("ä¸»æ¨¡å‹ä¸å…·å¤‡å¥çº§æ—¶é—´æˆ³èƒ½åŠ›ï¼Œå°†ç»§ç»­ä½¿ç”¨VADä½†è·³è¿‡å¥çº§æ—¶é—´æˆ³")

    return model_dir, model_configs, supports_timestamp

# åˆå§‹åŒ–FunASRæ¨¡å‹
MODEL_SUPPORTS_TIMESTAMP = False


def init_model():
    global model, MODEL_SUPPORTS_TIMESTAMP  # å£°æ˜ä½¿ç”¨å…¨å±€å˜é‡
    print("="*50)
    print("å¼€å§‹åˆå§‹åŒ–FunASRæ¨¡å‹...")
    print("æ­£åœ¨æ£€æµ‹GPUçŠ¶æ€...")
    
    try:
        # æ£€æµ‹GPUæ˜¯å¦å¯ç”¨
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print("\n" + "="*20 + " GPUæ£€æµ‹ä¿¡æ¯ " + "="*20)
        print(f"CUDAæ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        if device == "cuda":
            gpu_count = torch.cuda.device_count()
            print(f"å¯ç”¨GPUæ•°é‡: {gpu_count}")
            for i in range(gpu_count):
                print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
                print(f"GPU {i} æ€»å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
                print(f"GPU {i} CUDAç‰ˆæœ¬: {torch.version.cuda}")
                if hasattr(torch.backends.cudnn, 'version'):
                    print(f"GPU {i} cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        else:
            print("è­¦å‘Š: æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPUï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")
        
        print("\n" + "="*20 + " æ¨¡å‹åŠ è½½å¼€å§‹ " + "="*20)
        
        # ç¡®ä¿æ¨¡å‹å­˜åœ¨
        model_dir, model_info, supports_timestamp = ensure_models()
        MODEL_SUPPORTS_TIMESTAMP = supports_timestamp
        if MODEL_SUPPORTS_TIMESTAMP:
            logger.info("å½“å‰ä¸»æ¨¡å‹æ”¯æŒ sentence_timestamp è¾“å‡º")
        else:
            logger.warning("å½“å‰ä¸»æ¨¡å‹ä¸æ”¯æŒ sentence_timestampï¼Œå°†è·³è¿‡æ—¶é—´æˆ³å’Œè¯´è¯äººä¿¡æ¯")
        
        try:
            # å°è¯•ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
            init_kwargs = {
                "model": model_info["main"]["runtime"],
                "device": device,
                "model_dir": model_dir,
                "batch_size": 1 if device == "cpu" else 4,
                "disable_update": True,
                "use_local": True,
            }
            trust_remote_code = model_info["main"].get("requires_trust_remote_code", False)
            vad_config = model_info.get("vad")
            if vad_config:
                init_kwargs.update({
                    "vad_model": vad_config["runtime"],
                    "vad_kwargs": {"max_single_segment_time": 60000},
                    "vad_model_dir": model_dir,
                })
                trust_remote_code = trust_remote_code or vad_config.get("requires_trust_remote_code", False)
            punc_config = model_info.get("punc")
            if punc_config:
                init_kwargs.update({
                    "punc_model": punc_config["runtime"],
                    "punc_model_dir": model_dir,
                })
                trust_remote_code = trust_remote_code or punc_config.get("requires_trust_remote_code", False)
            spk_config = model_info.get("spk")
            if spk_config:
                init_kwargs.update({
                    "spk_model": spk_config["runtime"],
                    "spk_model_dir": model_dir,
                })
                trust_remote_code = trust_remote_code or spk_config.get("requires_trust_remote_code", False)

            if trust_remote_code:
                init_kwargs["trust_remote_code"] = True

            logger.info("FunASRåˆå§‹åŒ–å‚æ•°: %s", {k: v if k != "hotword" else "***" for k, v in init_kwargs.items()})
            model = AutoModel(**init_kwargs)
            print(f"FunASRæ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")
            print(f"ä¸»æ¨¡å‹: {model_info['main']['name']} ({model_info['main']['id']})")
            if vad_config:
                print(f"VADæ¨¡å‹: {vad_config['name']} ({vad_config['id']})")
            else:
                print("VADæ¨¡å‹: å·²ç¦ç”¨")
            if punc_config:
                print(f"æ ‡ç‚¹æ¨¡å‹: {punc_config['name']} ({punc_config['id']})")
            else:
                print("æ ‡ç‚¹æ¨¡å‹: å·²ç¦ç”¨")
            if spk_config:
                print(f"è¯´è¯äººæ¨¡å‹: {spk_config['name']} ({spk_config['id']})")
            else:
                print("è¯´è¯äººæ¨¡å‹: å·²ç¦ç”¨")
            print(f"æ‰¹å¤„ç†å¤§å°: {1 if device == 'cpu' else 4}")
            
        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½æŒ‡å®šæ¨¡å‹å¤±è´¥: {str(e)}")
            print("å°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹é…ç½®")
            
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹
            fallback_kwargs = {
                "model": "paraformer-zh",
                "device": device,
                "model_dir": model_dir,
                "batch_size": 1 if device == "cpu" else 4,
                "disable_update": True,
                "use_local": True,
            }
            fallback_kwargs.update({
                "vad_model": "fsmn-vad",
                "vad_kwargs": {"max_single_segment_time": 60000},
                "vad_model_dir": model_dir,
                "punc_model": "ct-punc",
                "punc_model_dir": model_dir,
                "spk_model": "cam++",
                "spk_model_dir": model_dir,
            })

            logger.warning("ä½¿ç”¨é»˜è®¤æ¨¡å‹å‚æ•°: %s", fallback_kwargs)
            model = AutoModel(**fallback_kwargs)
            MODEL_SUPPORTS_TIMESTAMP = False
            print(f"FunASRæ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")
            print(f"ä¸»æ¨¡å‹: paraformer-zh")
            print(f"VADæ¨¡å‹: fsmn-vad")
            print(f"æ ‡ç‚¹æ¨¡å‹: ct-punc")
            print(f"è¯´è¯äººæ¨¡å‹: cam++")
            print(f"æ‰¹å¤„ç†å¤§å°: {1 if device == 'cpu' else 4}")
        
        # éªŒè¯æ¨¡å‹åŠ è½½
        print("éªŒè¯æ¨¡å‹åŠ è½½çŠ¶æ€...")
        test_audio = np.zeros(16000, dtype=np.float32)  # 1ç§’çš„é™éŸ³ç”¨äºæµ‹è¯•
        test_result = model.generate(input=test_audio, sample_rate=16000)
        print(f"æ¨¡å‹éªŒè¯ç»“æœ: {test_result}")
        print("FunASRæ¨¡å‹åŠ è½½å®Œæˆ")
        
        return model
        
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½FunASRæ¨¡å‹å¤±è´¥: {str(e)}")
        import traceback
        print(traceback.format_exc())  # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        sys.exit(1)

# åˆå§‹åŒ–æ¨¡å‹
model = init_model()

@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    device_info = {
        "status": "healthy",
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "gpu_available": torch.cuda.is_available(),
        "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
        "gpu_names": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else [],
        "timestamp": datetime.now().isoformat()
    }
    return jsonify(device_info)

@app.route('/device')
def device_info():
    """è®¾å¤‡ä¿¡æ¯æ¥å£"""
    device_info = {
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "gpu_available": torch.cuda.is_available(),
        "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
        "gpu_names": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else [],
        "timestamp": datetime.now().isoformat()
    }
    return jsonify(device_info)

@app.route('/progress')
def get_progress():
    """è·å–å½“å‰è½¬å½•è¿›åº¦"""
    global current_progress
    
    # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
    if current_progress["start_time"] and current_progress["current_chunk"] > 0:
        elapsed_time = time.time() - current_progress["start_time"]
        avg_time_per_chunk = elapsed_time / current_progress["current_chunk"]
        remaining_chunks = current_progress["total_chunks"] - current_progress["current_chunk"]
        estimated_remaining = avg_time_per_chunk * remaining_chunks
        current_progress["estimated_time"] = estimated_remaining
    
    return jsonify(current_progress)

def update_progress(status, current_chunk=None, total_chunks=None, message=None):
    """æ›´æ–°è¿›åº¦ä¿¡æ¯"""
    global current_progress
    
    current_progress["status"] = status
    if current_chunk is not None:
        current_progress["current_chunk"] = current_chunk
    if total_chunks is not None:
        current_progress["total_chunks"] = total_chunks
    if message is not None:
        current_progress["message"] = message
    
    if total_chunks and total_chunks > 0:
        current_progress["progress"] = (current_progress["current_chunk"] / total_chunks) * 100
    
    if status == "processing" and current_progress["start_time"] is None:
        current_progress["start_time"] = time.time()
    elif status == "completed" or status == "error":
        current_progress["start_time"] = None
        current_progress["estimated_time"] = None

def convert_audio_to_wav(input_path, target_sample_rate=16000):
    """å°†éŸ³é¢‘è½¬æ¢ä¸ºWAVæ ¼å¼å¹¶é‡é‡‡æ ·"""
    try:
        # ä½¿ç”¨pydubåŠ è½½éŸ³é¢‘
        logger.info(f"å¼€å§‹è½¬æ¢éŸ³é¢‘æ–‡ä»¶: {input_path}")
        audio = AudioSegment.from_file(input_path)
        
        logger.info(f"åŸå§‹éŸ³é¢‘ä¿¡æ¯: é€šé“æ•°={audio.channels}, é‡‡æ ·ç‡={audio.frame_rate}Hz, æ—¶é•¿={len(audio)/1000.0}ç§’")
        
        # è½¬æ¢ä¸ºå•å£°é“
        if audio.channels > 1:
            audio = audio.set_channels(1)
            logger.info("å·²è½¬æ¢ä¸ºå•å£°é“")
        
        # è®¾ç½®é‡‡æ ·ç‡
        if audio.frame_rate != target_sample_rate:
            audio = audio.set_frame_rate(target_sample_rate)
            logger.info(f"å·²è°ƒæ•´é‡‡æ ·ç‡è‡³ {target_sample_rate}Hz")
        
        # è°ƒæ•´éŸ³é‡
        if audio.dBFS < -30:  # å¦‚æœéŸ³é‡å¤ªå°
            gain_needed = min(-30 - audio.dBFS, 30)  # æœ€å¤šå¢ç›Š30dB
            audio = audio.apply_gain(gain_needed)
            logger.info(f"éŸ³é‡è¿‡å°ï¼Œå·²å¢åŠ  {gain_needed}dB")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_wav = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        temp_wav_path = temp_wav.name
        temp_wav.close()
        
        # å¯¼å‡ºä¸ºWAVæ ¼å¼
        audio.export(temp_wav_path, format='wav', parameters=["-ac", "1", "-ar", str(target_sample_rate)])
        logger.info(f"éŸ³é¢‘å·²å¯¼å‡ºä¸ºWAVæ ¼å¼: {temp_wav_path}")
        
        return temp_wav_path
    except Exception as e:
        logger.error(f"éŸ³é¢‘è½¬æ¢å¤±è´¥: {str(e)}")
        raise

def normalize_audio(audio_data):
    """æ ‡å‡†åŒ–éŸ³é¢‘æ•°æ®"""
    try:
        if audio_data.dtype != np.float32:
            audio_data = audio_data.astype(np.float32)
        
        # è®¡ç®—éŸ³é¢‘ç»Ÿè®¡ä¿¡æ¯
        mean_val = np.mean(audio_data)
        std_val = np.std(audio_data)
        max_abs = np.max(np.abs(audio_data))
        
        logger.info(f"éŸ³é¢‘ç»Ÿè®¡: å‡å€¼={mean_val:.6f}, æ ‡å‡†å·®={std_val:.6f}, æœ€å¤§ç»å¯¹å€¼={max_abs:.6f}")
        
        # å¦‚æœéŸ³é¢‘å¤ªå¼±ï¼Œè¿›è¡Œæ”¾å¤§
        if max_abs < 0.1:
            scale_factor = 0.5 / max_abs  # æ”¾å¤§åˆ°0.5çš„å¹…åº¦
            audio_data = audio_data * scale_factor
            logger.info(f"éŸ³é¢‘ä¿¡å·è¾ƒå¼±ï¼Œå·²æ”¾å¤§ {scale_factor:.2f} å€")
        
        # ç¡®ä¿éŸ³é¢‘æ•°æ®åœ¨ [-1, 1] èŒƒå›´å†…
        if max_abs > 1.0:
            audio_data = audio_data / max_abs
            logger.info("éŸ³é¢‘æ•°æ®å·²å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´")
        
        # ç§»é™¤DCåç½®
        audio_data = audio_data - np.mean(audio_data)
        
        return audio_data
    except Exception as e:
        logger.error(f"éŸ³é¢‘æ ‡å‡†åŒ–å¤±è´¥: {str(e)}")
        raise

def _convert_timestamp_value(value):
    """FunASRæ—¶é—´æˆ³è½¬æ¢ä¸ºç§’"""
    if value is None:
        return None
    if isinstance(value, (int, float)):
        if value > 1000:
            return value / 1000.0
        return float(value)
    return None


def process_recognition_result(result):
    """å¤„ç†è¯†åˆ«ç»“æœï¼Œè¿”å›ç»Ÿä¸€ç»“æ„"""
    try:
        logger.debug(f"å¤„ç†è¯†åˆ«ç»“æœ: {type(result)} - {result}")

        combined_text_parts = []
        sentence_info = []
        raw_segments = []

        if isinstance(result, str):
            combined_text_parts.append(result.strip())
        elif isinstance(result, list):
            if len(result) == 0:
                logger.warning("è¯†åˆ«ç»“æœåˆ—è¡¨ä¸ºç©º")
            for i, item in enumerate(result):
                try:
                    if isinstance(item, dict):
                        text = (item.get('text') or item.get('result') or item.get('sentence') or '').strip()
                        if text:
                            combined_text_parts.append(text)

                        start = _convert_timestamp_value(item.get('start'))
                        end = _convert_timestamp_value(item.get('end'))

                        token_timestamps = item.get('timestamp', [])
                        if token_timestamps:
                            first_ts = token_timestamps[0]
                            last_ts = token_timestamps[-1]
                            if start is None and len(first_ts) >= 1:
                                start = _convert_timestamp_value(first_ts[0])
                            if end is None and len(last_ts) >= 2:
                                end = _convert_timestamp_value(last_ts[1])

                        if start is not None and end is not None and text:
                            sentence_info.append({
                                'text': text,
                                'start': start,
                                'end': end,
                                'word_timestamps': [
                                    [
                                        _convert_timestamp_value(ts[0]),
                                        _convert_timestamp_value(ts[1])
                                    ]
                                    for ts in token_timestamps
                                    if isinstance(ts, (list, tuple)) and len(ts) >= 2
                                ]
                            })
                        raw_segments.append(item)
                    elif isinstance(item, str):
                        combined_text_parts.append(item.strip())
                    else:
                        logger.debug(f"åˆ—è¡¨é¡¹ {i}: æœªçŸ¥æ ¼å¼ {type(item)} - {item}")
                except Exception as e:
                    logger.warning(f"å¤„ç†åˆ—è¡¨é¡¹ {i} æ—¶å‡ºé”™: {str(e)}")
                    continue
        elif isinstance(result, dict):
            text = (result.get('text') or result.get('result') or result.get('sentence') or '').strip()
            if text:
                combined_text_parts.append(text)

            sentence_list = result.get('sentence_info')
            if isinstance(sentence_list, list):
                for sentence in sentence_list:
                    try:
                        sent_text = (sentence.get('text') or '').strip()
                        start = _convert_timestamp_value(sentence.get('start'))
                        end = _convert_timestamp_value(sentence.get('end'))
                        if sent_text and start is not None and end is not None:
                            sentence_info.append({
                                'text': sent_text,
                                'start': start,
                                'end': end,
                                'word_timestamps': [
                                    [
                                        _convert_timestamp_value(ts[0]),
                                        _convert_timestamp_value(ts[1])
                                    ]
                                    for ts in sentence.get('timestamp', [])
                                    if isinstance(ts, (list, tuple)) and len(ts) >= 2
                                ]
                            })
                    except Exception as e:
                        logger.warning(f"å¤„ç†sentence_infoæ—¶å‡ºé”™: {str(e)}")
            raw_segments.append(result)
        elif result is None:
            combined_text_parts.append("")
        else:
            logger.warning(f"æœªçŸ¥çš„è¯†åˆ«ç»“æœæ ¼å¼: {type(result)} - {result}")
            if result:
                combined_text_parts.append(str(result))

        combined_text = " ".join(filter(None, combined_text_parts)).strip()
        return {
            'text': combined_text,
            'sentence_info': sentence_info,
            'raw_segments': raw_segments
        }
    except Exception as e:
        logger.error(f"å¤„ç†è¯†åˆ«ç»“æœæ—¶å‡ºé”™: {str(e)}")
        return {
            'text': "",
            'sentence_info': [],
            'raw_segments': []
        }

def process_audio_chunk(audio_data, sample_rate, chunk_size=30*16000, hotwords=None):
    """åˆ†å—å¤„ç†éŸ³é¢‘æ•°æ®
    
    Args:
        audio_data: éŸ³é¢‘æ•°æ®
        sample_rate: é‡‡æ ·ç‡
        chunk_size: æ¯ä¸ªéŸ³é¢‘å—çš„å¤§å°ï¼ˆé»˜è®¤30ç§’ï¼‰
        hotwords: çƒ­è¯åˆ—è¡¨ï¼Œç”¨äºæé«˜ç‰¹å®šè¯æ±‡çš„è¯†åˆ«å‡†ç¡®ç‡
    """
    try:
        results = []
        total_len = len(audio_data)
        
        # æ ‡å‡†åŒ–éŸ³é¢‘æ•°æ®
        audio_data = normalize_audio(audio_data)
        
        # å¦‚æœéŸ³é¢‘å¤ªçŸ­ï¼Œç›´æ¥å¤„ç†æ•´ä¸ªéŸ³é¢‘
        if total_len < chunk_size:
            update_progress("processing", 0, 1, "å¤„ç†çŸ­éŸ³é¢‘...")
            try:
                with torch.no_grad():
                    # print(f"\nå¤„ç†çŸ­éŸ³é¢‘ (é•¿åº¦: {total_len/sample_rate:.2f}ç§’)")
                    # ä¿®å¤çƒ­è¯å‚æ•°ä¼ é€’
                    if hotwords:
                        # FunASRå®˜æ–¹æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨hotwordå‚æ•°ï¼Œç©ºæ ¼åˆ†éš”å¤šä¸ªçƒ­è¯
                        hotword_string = ' '.join(hotwords)
                        logger.warning(f"ğŸ”¥ çŸ­éŸ³é¢‘ä½¿ç”¨çƒ­è¯: '{hotword_string}'")
                        result = model.generate(
                            input=audio_data,
                            hotword=hotword_string
                        )
                    else:
                        logger.warning("ğŸ”¥ çŸ­éŸ³é¢‘è°ƒç”¨ model.generateï¼Œæ— çƒ­è¯")
                        result = model.generate(
                            input=audio_data
                        )
                    processed_result = process_recognition_result(result)
                    chunk_text = processed_result.get('text', '')

                    if chunk_text:
                        results.append(chunk_text)
                    update_progress("processing", 1, 1, "çŸ­éŸ³é¢‘å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"å¤„ç†çŸ­éŸ³é¢‘æ—¶å‡ºé”™: {str(e)}")
                update_progress("error", message=f"çŸ­éŸ³é¢‘å¤„ç†é”™è¯¯: {str(e)}")
        else:
            # åˆ†å—å¤„ç†é•¿éŸ³é¢‘
            overlap = int(0.5 * sample_rate)  # 0.5ç§’é‡å 
            total_chunks = (total_len + chunk_size - 1)//chunk_size
            print(f"\nå¼€å§‹å¤„ç†éŸ³é¢‘ï¼Œæ€»å…± {total_chunks} ä¸ªå—")
            update_progress("processing", 0, total_chunks, f"å¼€å§‹å¤„ç† {total_chunks} ä¸ªéŸ³é¢‘å—...")
            
            for i in range(0, total_len, chunk_size - overlap):
                chunk = audio_data[i:min(i+chunk_size, total_len)]
                chunk_num = i//chunk_size + 1
                
                update_progress("processing", chunk_num, total_chunks, f"æ­£åœ¨å¤„ç†ç¬¬ {chunk_num}/{total_chunks} ä¸ªéŸ³é¢‘å—...")
                
                # æ£€æŸ¥éŸ³é¢‘å—çš„æœ‰æ•ˆæ€§
                chunk_max = float(np.max(np.abs(chunk))) if chunk.size else 0.0
                chunk_energy = float(np.mean(chunk**2)) if chunk.size else 0.0

                # æ›´ä¿å®ˆåœ°åˆ¤æ–­é™éŸ³ï¼Œé¿å…å°†ä½éŸ³é‡è¯­éŸ³å½“ä½œå™ªå£°è·³è¿‡
                silence_peak_threshold = 1e-4
                silence_energy_threshold = 1e-8
                if chunk_max < silence_peak_threshold and chunk_energy < silence_energy_threshold:
                    logger.debug(
                        "è·³è¿‡é™éŸ³å— %s/%s (å³°å€¼=%.6e, èƒ½é‡=%.6e)",
                        chunk_num,
                        total_chunks,
                        chunk_max,
                        chunk_energy,
                    )
                    continue
                
                try:
                    with torch.no_grad():
                        # ä¿®å¤çƒ­è¯å‚æ•°ä¼ é€’
                        kwargs = {}
                        if hotwords:
                            hotword_string = ' '.join(hotwords)
                            logger.warning(f"ğŸ”¥ é•¿éŸ³é¢‘å—{chunk_num}ä½¿ç”¨çƒ­è¯: '{hotword_string}'")
                            kwargs['hotword'] = hotword_string
                        if not MODEL_SUPPORTS_TIMESTAMP:
                            kwargs['sentence_timestamp'] = False
                        logger.debug(
                            "è°ƒç”¨æ¨¡å‹å¤„ç†å— %s/%sï¼Œkwargs=%s",
                            chunk_num,
                            total_chunks,
                            {k: ("***" if k == "hotword" else v) for k, v in kwargs.items()},
                        )
                        result = model.generate(
                            input=chunk,
                            **kwargs
                        )
                        processed_result = process_recognition_result(result)
                        chunk_text = processed_result.get('text', '')

                        if chunk_text:
                            results.append(chunk_text)
                            # print(f"è¯†åˆ«ç»“æœ: {processed_result}")
                except Exception as e:
                    print(f"å¤„ç†éŸ³é¢‘å— {chunk_num} æ—¶å‡ºé”™: {str(e)}")
                    continue
        
        final_result = " ".join(results)
        print("\néŸ³é¢‘å¤„ç†å®Œæˆï¼")
        update_progress("completed", message="éŸ³é¢‘å¤„ç†å®Œæˆï¼")
        return final_result
        
    except Exception as e:
        print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")
        raise

@app.route('/recognize', methods=['POST'])
def recognize_audio():
    """å¤„ç†éŸ³é¢‘æ–‡ä»¶å¹¶è¿”å›è¯†åˆ«ç»“æœ"""
    temp_files = []  # ç”¨äºè·Ÿè¸ªéœ€è¦æ¸…ç†çš„ä¸´æ—¶æ–‡ä»¶
    
    try:
        # åˆå§‹åŒ–è¿›åº¦
        update_progress("starting", 0, 0, "å¼€å§‹å¤„ç†éŸ³é¢‘æ–‡ä»¶...")
        
        if 'audio' not in request.files:
            update_progress("error", message="æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            return jsonify({"error": "æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶"}), 400
            
        audio_file = request.files['audio']
        if not audio_file:
            return jsonify({"error": "ç©ºçš„éŸ³é¢‘æ–‡ä»¶"}), 400
        
        # è·å–åŸå§‹æ–‡ä»¶ä¿¡æ¯
        original_filename = audio_file.filename
        file_size = len(audio_file.read())
        audio_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        logger.info(f"æ¥æ”¶åˆ°éŸ³é¢‘æ–‡ä»¶: {original_filename}, å¤§å°: {file_size/1024:.2f}KB")
        
        # è·å–çƒ­è¯å‚æ•°
        hotwords_raw = request.form.get('hotwords', '')
        logger.warning(f"ğŸ”¥ FunASRæ¥æ”¶åˆ°åŸå§‹çƒ­è¯å­—ç¬¦ä¸²: '{hotwords_raw}'")
        
        if hotwords_raw:
            hotwords = [word.strip() for word in hotwords_raw.split(',') if word.strip()]
            logger.warning(f"ğŸ”¥ FunASRè§£æåçš„çƒ­è¯åˆ—è¡¨ ({len(hotwords)}ä¸ª): {hotwords}")
        else:
            hotwords = []
            logger.warning("ğŸ”¥ FunASRæ²¡æœ‰æ¥æ”¶åˆ°çƒ­è¯å‚æ•°")
        
        # ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
        orig_audio_path = os.path.join(app.config['UPLOAD_FOLDER'], 'temp_audio_orig')
        audio_file.save(orig_audio_path)
        temp_files.append(orig_audio_path)
        
        try:
            # è½¬æ¢éŸ³é¢‘æ ¼å¼
            wav_path = convert_audio_to_wav(orig_audio_path)
            temp_files.append(wav_path)
            
            # è¯»å–è½¬æ¢åçš„éŸ³é¢‘æ–‡ä»¶
            audio_data, sample_rate = sf.read(wav_path)
            
            # æ£€æŸ¥éŸ³é¢‘æ•°æ®æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
            if len(audio_data) == 0:
                logger.error("éŸ³é¢‘æ•°æ®ä¸ºç©º")
                return jsonify({"error": "éŸ³é¢‘æ•°æ®ä¸ºç©º"}), 400
                
            if np.all(np.abs(audio_data) < 1e-6):
                logger.error("éŸ³é¢‘æ•°æ®å…¨ä¸ºé™éŸ³")
                return jsonify({"error": "éŸ³é¢‘æ•°æ®å…¨ä¸ºé™éŸ³"}), 400
            
            logger.info("å¼€å§‹éŸ³é¢‘è¯†åˆ«...")

            hotword_string = ' '.join(hotwords) if hotwords else None
            generation_kwargs = {}
            if hotword_string:
                generation_kwargs['hotword'] = hotword_string
            if MODEL_SUPPORTS_TIMESTAMP:
                generation_kwargs['sentence_timestamp'] = True
            logger.debug(
                "æ•´ä½“è¯†åˆ«è°ƒç”¨å‚æ•°: supports_timestamp=%s kwargs=%s",
                MODEL_SUPPORTS_TIMESTAMP,
                {k: ("***" if k == "hotword" else v) for k, v in generation_kwargs.items()},
            )

            parsed_result = None
            raw_result = None

            if MODEL_SUPPORTS_TIMESTAMP:
                try:
                    with torch.no_grad():
                        logger.info("è°ƒç”¨FunASRæ•´ä½“è¯†åˆ«ï¼ˆå¸¦VADï¼‰")
                        raw_result = model.generate(
                            input=audio_data,
                            **generation_kwargs
                        )
                    parsed_result = process_recognition_result(raw_result)
                except Exception as e:
                    logger.error(f"æ•´ä½“è¯†åˆ«ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰å¤±è´¥: {str(e)}")
                    logger.exception(e)
                    parsed_result = None
            else:
                logger.info("å½“å‰æ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³ï¼Œè·³è¿‡æ•´ä½“è¯†åˆ«ï¼Œç›´æ¥è¿›å…¥åˆ†å—æµç¨‹")

            if not parsed_result or not parsed_result.get('text'):
                logger.warning("æ•´ä½“è¯†åˆ«ç»“æœä¸ºç©ºï¼Œå°è¯•åˆ†å—å¤„ç†")
                chunk_text = process_audio_chunk(
                    audio_data,
                    sample_rate,
                    hotwords=hotwords if hotwords else None
                )
                if chunk_text and chunk_text.strip():
                    parsed_result = {
                        'text': chunk_text.strip(),
                        'sentence_info': [],
                        'raw_segments': []
                    }
                elif MODEL_SUPPORTS_TIMESTAMP:
                    logger.warning("åˆ†å—è¯†åˆ«ä»ä¸ºç©ºï¼Œå°è¯•æ— æ—¶é—´æˆ³çš„æ•´ä½“è¯†åˆ«")
                    try:
                        fallback_kwargs = generation_kwargs.copy()
                        fallback_kwargs.pop('sentence_timestamp', None)
                        logger.debug(
                            "æ— æ—¶é—´æˆ³æ•´ä½“è¯†åˆ«å‚æ•°: %s",
                            {k: ("***" if k == "hotword" else v) for k, v in fallback_kwargs.items()},
                        )
                        with torch.no_grad():
                            raw_result = model.generate(
                                input=audio_data,
                                **fallback_kwargs
                            )
                        parsed_result = process_recognition_result(raw_result)
                    except Exception as e:
                        logger.error(f"æ— æ—¶é—´æˆ³æ•´ä½“è¯†åˆ«å¤±è´¥: {str(e)}")
                        logger.exception(e)
                        parsed_result = {
                            'text': '',
                            'sentence_info': [],
                            'raw_segments': []
                        }
                else:
                    logger.warning("åˆ†å—è¯†åˆ«ä»ä¸ºç©ºï¼Œæ¨¡å‹ä¸æ”¯æŒæ—¶é—´æˆ³ï¼Œå°†è¿”å›ç©ºç»“æœ")
                    parsed_result = {
                        'text': '',
                        'sentence_info': [],
                        'raw_segments': []
                    }

            text_output = parsed_result.get('text', '').strip()
            parsed_result['text'] = text_output

            logger.info("éŸ³é¢‘è¯†åˆ«å®Œæˆ")

            sentence_info = parsed_result.get('sentence_info', [])

            response_data = {
                "success": True,
                "text": text_output,
                "audio_info": {
                    "original_filename": original_filename,
                    "file_size_kb": file_size/1024,
                    "duration_seconds": len(audio_data)/sample_rate,
                    "sample_rate": sample_rate
                },
                "sentence_info": sentence_info,
                "timestamp": sentence_info
            }
            return jsonify(response_data)
            
        except Exception as e:
            logger.error(f"å¤„ç†éŸ³é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return jsonify({"error": f"å¤„ç†éŸ³é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"}), 500
            
    except Exception as e:
        logger.error(f"è¯·æ±‚å¤„ç†å‡ºé”™: {str(e)}")
        return jsonify({"error": f"è¯·æ±‚å¤„ç†å‡ºé”™: {str(e)}"}), 500
        
    finally:
        # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception as e:
                logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")

if __name__ == '__main__':
    try:
        # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
        os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
        
        print("\n" + "="*20 + " FunASRæœåŠ¡å¯åŠ¨ " + "="*20)
        print(f"æœåŠ¡å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ç›‘å¬åœ°å€: 0.0.0.0:{10095}")
        print(f"ä¸Šä¼ ç›®å½•: {app.config['UPLOAD_FOLDER']}")
        print(f"æ¨¡å‹ç›®å½•: {os.getenv('MODEL_DIR', '/app/models')}")
        print("="*56 + "\n")
        
        # å¯åŠ¨Flaskåº”ç”¨
        app.run(host='0.0.0.0', port=10095, threaded=True)
    except Exception as e:
        print(f"é”™è¯¯: å¯åŠ¨FunASRæœåŠ¡å¤±è´¥: {str(e)}")
        import traceback
        print(traceback.format_exc())
        sys.exit(1)
